loading file tokenizer.json from cache at /data/b22cs093/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/tokenizer.json
loading file tokenizer.model from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /data/b22cs093/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/special_tokens_map.json
loading file tokenizer_config.json from cache at /data/b22cs093/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/tokenizer_config.json
loading file chat_template.jinja from cache at None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading configuration file config.json from cache at /data/b22cs093/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "dtype": "bfloat16",
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.3",
  "use_cache": true,
  "vocab_size": 128256
}

Overriding dtype=None with `dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own dtype to specify the dtype of the remaining non-linear layers or pass dtype=torch.float16 to remove this warning.
loading weights file model.safetensors from cache at /data/b22cs093/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/model.safetensors
Instantiating LlamaForCausalLM model under default dtype torch.float16.
Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ]
}

target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization
loading configuration file generation_config.json from cache at /data/b22cs093/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "temperature": 0.6,
  "top_p": 0.9
}

Could not locate the custom_generate/generate.py inside meta-llama/Llama-3.2-1B-Instruct.
Device set to use cuda:0
loading configuration file config.json from cache at /data/b22cs093/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "dtype": "bfloat16",
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 16,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 32.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.3",
  "use_cache": true,
  "vocab_size": 128256
}

Overriding dtype=None with `dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own dtype to specify the dtype of the remaining non-linear layers or pass dtype=torch.float16 to remove this warning.
loading weights file model.safetensors from cache at /data/b22cs093/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/model.safetensors
Instantiating LlamaForCausalLM model under default dtype torch.float16.
Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ]
}

target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization
loading configuration file generation_config.json from cache at /data/b22cs093/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "temperature": 0.6,
  "top_p": 0.9
}

Could not locate the custom_generate/generate.py inside meta-llama/Llama-3.2-1B-Instruct.
Device set to use cuda:0
loading file vocab.json from cache at /data/b22cs093/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM-360M-Instruct/snapshots/73b7144f76331266f5f45d5642fd8da653583b13/vocab.json
loading file merges.txt from cache at /data/b22cs093/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM-360M-Instruct/snapshots/73b7144f76331266f5f45d5642fd8da653583b13/merges.txt
loading file tokenizer.json from cache at /data/b22cs093/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM-360M-Instruct/snapshots/73b7144f76331266f5f45d5642fd8da653583b13/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /data/b22cs093/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM-360M-Instruct/snapshots/73b7144f76331266f5f45d5642fd8da653583b13/special_tokens_map.json
loading file tokenizer_config.json from cache at /data/b22cs093/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM-360M-Instruct/snapshots/73b7144f76331266f5f45d5642fd8da653583b13/tokenizer_config.json
loading file chat_template.jinja from cache at None
loading configuration file config.json from cache at /data/b22cs093/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM-360M-Instruct/snapshots/73b7144f76331266f5f45d5642fd8da653583b13/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 960,
  "initializer_range": 0.02,
  "intermediate_size": 2560,
  "max_position_embeddings": 2048,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 15,
  "num_hidden_layers": 32,
  "num_key_value_heads": 5,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.3",
  "use_cache": true,
  "vocab_size": 49152
}

Overriding dtype=None with `dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own dtype to specify the dtype of the remaining non-linear layers or pass dtype=torch.float16 to remove this warning.
loading weights file model.safetensors from cache at /data/b22cs093/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM-360M-Instruct/snapshots/73b7144f76331266f5f45d5642fd8da653583b13/model.safetensors
Instantiating LlamaForCausalLM model under default dtype torch.float16.
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 2
}

target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization
loading configuration file generation_config.json from cache at /data/b22cs093/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM-360M-Instruct/snapshots/73b7144f76331266f5f45d5642fd8da653583b13/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_new_tokens": 40,
  "pad_token_id": 2
}

Could not locate the custom_generate/generate.py inside HuggingFaceTB/SmolLM-360M-Instruct.
Device set to use cuda:0
loading configuration file config.json from cache at /data/b22cs093/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM-360M-Instruct/snapshots/73b7144f76331266f5f45d5642fd8da653583b13/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 960,
  "initializer_range": 0.02,
  "intermediate_size": 2560,
  "max_position_embeddings": 2048,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 15,
  "num_hidden_layers": 32,
  "num_key_value_heads": 5,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.3",
  "use_cache": true,
  "vocab_size": 49152
}

Overriding dtype=None with `dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own dtype to specify the dtype of the remaining non-linear layers or pass dtype=torch.float16 to remove this warning.
loading weights file model.safetensors from cache at /data/b22cs093/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM-360M-Instruct/snapshots/73b7144f76331266f5f45d5642fd8da653583b13/model.safetensors
Instantiating LlamaForCausalLM model under default dtype torch.float16.
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 2
}

target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization
loading configuration file generation_config.json from cache at /data/b22cs093/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM-360M-Instruct/snapshots/73b7144f76331266f5f45d5642fd8da653583b13/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_new_tokens": 40,
  "pad_token_id": 2
}

Could not locate the custom_generate/generate.py inside HuggingFaceTB/SmolLM-360M-Instruct.
Device set to use cuda:0
loading file vocab.json from cache at /data/b22cs093/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM-360M-Instruct/snapshots/73b7144f76331266f5f45d5642fd8da653583b13/vocab.json
loading file merges.txt from cache at /data/b22cs093/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM-360M-Instruct/snapshots/73b7144f76331266f5f45d5642fd8da653583b13/merges.txt
loading file tokenizer.json from cache at /data/b22cs093/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM-360M-Instruct/snapshots/73b7144f76331266f5f45d5642fd8da653583b13/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /data/b22cs093/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM-360M-Instruct/snapshots/73b7144f76331266f5f45d5642fd8da653583b13/special_tokens_map.json
loading file tokenizer_config.json from cache at /data/b22cs093/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM-360M-Instruct/snapshots/73b7144f76331266f5f45d5642fd8da653583b13/tokenizer_config.json
loading file chat_template.jinja from cache at None
loading configuration file config.json from cache at /data/b22cs093/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM-360M-Instruct/snapshots/73b7144f76331266f5f45d5642fd8da653583b13/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 960,
  "initializer_range": 0.02,
  "intermediate_size": 2560,
  "max_position_embeddings": 2048,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 15,
  "num_hidden_layers": 32,
  "num_key_value_heads": 5,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.3",
  "use_cache": true,
  "vocab_size": 49152
}

Overriding dtype=None with `dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own dtype to specify the dtype of the remaining non-linear layers or pass dtype=torch.float16 to remove this warning.
loading weights file model.safetensors from cache at /data/b22cs093/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM-360M-Instruct/snapshots/73b7144f76331266f5f45d5642fd8da653583b13/model.safetensors
Instantiating LlamaForCausalLM model under default dtype torch.float16.
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 2
}

target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization
loading configuration file generation_config.json from cache at /data/b22cs093/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM-360M-Instruct/snapshots/73b7144f76331266f5f45d5642fd8da653583b13/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_new_tokens": 40,
  "pad_token_id": 2
}

Could not locate the custom_generate/generate.py inside HuggingFaceTB/SmolLM-360M-Instruct.
Device set to use cuda:0
loading configuration file config.json from cache at /data/b22cs093/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM-360M-Instruct/snapshots/73b7144f76331266f5f45d5642fd8da653583b13/config.json
Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 960,
  "initializer_range": 0.02,
  "intermediate_size": 2560,
  "max_position_embeddings": 2048,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 15,
  "num_hidden_layers": 32,
  "num_key_value_heads": 5,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.3",
  "use_cache": true,
  "vocab_size": 49152
}

Overriding dtype=None with `dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own dtype to specify the dtype of the remaining non-linear layers or pass dtype=torch.float16 to remove this warning.
loading weights file model.safetensors from cache at /data/b22cs093/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM-360M-Instruct/snapshots/73b7144f76331266f5f45d5642fd8da653583b13/model.safetensors
Instantiating LlamaForCausalLM model under default dtype torch.float16.
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 2
}

target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization
loading configuration file generation_config.json from cache at /data/b22cs093/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM-360M-Instruct/snapshots/73b7144f76331266f5f45d5642fd8da653583b13/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2,
  "max_new_tokens": 40,
  "pad_token_id": 2
}

Could not locate the custom_generate/generate.py inside HuggingFaceTB/SmolLM-360M-Instruct.
Device set to use cuda:0
loading file vocab.json from cache at /data/b22cs093/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/vocab.json
loading file merges.txt from cache at /data/b22cs093/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/merges.txt
loading file tokenizer.json from cache at /data/b22cs093/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /data/b22cs093/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/tokenizer_config.json
loading file chat_template.jinja from cache at None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading configuration file config.json from cache at /data/b22cs093/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/config.json
Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 896,
  "initializer_range": 0.02,
  "intermediate_size": 4864,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 14,
  "num_hidden_layers": 24,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.3",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

Overriding dtype=None with `dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own dtype to specify the dtype of the remaining non-linear layers or pass dtype=torch.float16 to remove this warning.
loading weights file model.safetensors from cache at /data/b22cs093/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/model.safetensors
Instantiating Qwen2ForCausalLM model under default dtype torch.float16.
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization
loading configuration file generation_config.json from cache at /data/b22cs093/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.1,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

Could not locate the custom_generate/generate.py inside Qwen/Qwen2.5-0.5B-Instruct.
Device set to use cuda:0
loading configuration file config.json from cache at /data/b22cs093/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/config.json
Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 896,
  "initializer_range": 0.02,
  "intermediate_size": 4864,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 14,
  "num_hidden_layers": 24,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "transformers_version": "4.57.3",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

Overriding dtype=None with `dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own dtype to specify the dtype of the remaining non-linear layers or pass dtype=torch.float16 to remove this warning.
loading weights file model.safetensors from cache at /data/b22cs093/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/model.safetensors
Instantiating Qwen2ForCausalLM model under default dtype torch.float16.
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization
loading configuration file generation_config.json from cache at /data/b22cs093/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.1,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

Could not locate the custom_generate/generate.py inside Qwen/Qwen2.5-0.5B-Instruct.
Device set to use cuda:0
loading file tokenizer.model from cache at /data/b22cs093/.cache/huggingface/hub/models--microsoft--Phi-3-medium-4k-instruct/snapshots/b64223aaea6fbf273c0c8cd0801d5e732dce8897/tokenizer.model
loading file tokenizer.json from cache at /data/b22cs093/.cache/huggingface/hub/models--microsoft--Phi-3-medium-4k-instruct/snapshots/b64223aaea6fbf273c0c8cd0801d5e732dce8897/tokenizer.json
loading file added_tokens.json from cache at /data/b22cs093/.cache/huggingface/hub/models--microsoft--Phi-3-medium-4k-instruct/snapshots/b64223aaea6fbf273c0c8cd0801d5e732dce8897/added_tokens.json
loading file special_tokens_map.json from cache at /data/b22cs093/.cache/huggingface/hub/models--microsoft--Phi-3-medium-4k-instruct/snapshots/b64223aaea6fbf273c0c8cd0801d5e732dce8897/special_tokens_map.json
loading file tokenizer_config.json from cache at /data/b22cs093/.cache/huggingface/hub/models--microsoft--Phi-3-medium-4k-instruct/snapshots/b64223aaea6fbf273c0c8cd0801d5e732dce8897/tokenizer_config.json
loading file chat_template.jinja from cache at None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading configuration file config.json from cache at /data/b22cs093/.cache/huggingface/hub/models--microsoft--Phi-3-medium-4k-instruct/snapshots/b64223aaea6fbf273c0c8cd0801d5e732dce8897/config.json
loading configuration file config.json from cache at /data/b22cs093/.cache/huggingface/hub/models--microsoft--Phi-3-medium-4k-instruct/snapshots/b64223aaea6fbf273c0c8cd0801d5e732dce8897/config.json
Model config Phi3Config {
  "architectures": [
    "Phi3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_phi3.Phi3Config",
    "AutoModelForCausalLM": "modeling_phi3.Phi3ForCausalLM"
  },
  "bos_token_id": 1,
  "dtype": "bfloat16",
  "embd_pdrop": 0.0,
  "eos_token_id": 32000,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 17920,
  "max_position_embeddings": 4096,
  "model_type": "phi3",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 10,
  "original_max_position_embeddings": 4096,
  "pad_token_id": 32000,
  "resid_pdrop": 0.0,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "sliding_window": 2047,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.3",
  "use_cache": true,
  "vocab_size": 32064
}

`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
Overriding dtype=None with `dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own dtype to specify the dtype of the remaining non-linear layers or pass dtype=torch.float16 to remove this warning.
loading weights file model.safetensors from cache at /data/b22cs093/.cache/huggingface/hub/models--microsoft--Phi-3-medium-4k-instruct/snapshots/b64223aaea6fbf273c0c8cd0801d5e732dce8897/model.safetensors.index.json
Instantiating Phi3ForCausalLM model under default dtype torch.float16.
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 32000,
  "pad_token_id": 32000
}

target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization

=== Processing meta-llama/Llama-3.2-1B-Instruct ===
Adapter: ./models/llama-3.2-1b-tool-calling-final
Loading Base Model...
Loading Fine-Tuned Model...

=== Processing HuggingFaceTB/SmolLM-360M-Instruct ===
Adapter: ./models/smollm-360m-tool-calling-final
Loading Base Model...
Loading Fine-Tuned Model...

=== Processing HuggingFaceTB/SmolLM-360M-Instruct ===
Adapter: ./models/smollm-360m-tool-calling-dpo-final
Loading Base Model...
Loading Fine-Tuned Model...

=== Processing Qwen/Qwen2.5-0.5B-Instruct ===
Adapter: ./models/qwen2.5-0.5b-tool-calling-final
Loading Base Model...
Loading Fine-Tuned Model...

=== Processing Qwen/Qwen2.5-1.5B-Instruct ===
Adapter: ./models/qwen2.5-1.5b-tool-calling-final
Warning: Adapter path ./models/qwen2.5-1.5b-tool-calling-final does not exist. Skipping.

=== Processing microsoft/Phi-3-medium-4k-instruct ===
Adapter: ./models/phi3-medium-tool-calling-final
Loading Base Model...
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:03<00:16,  3.38s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:06<00:13,  3.44s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:10<00:10,  3.45s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:13<00:06,  3.43s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:17<00:03,  3.40s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:19<00:00,  3.03s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:19<00:00,  3.23s/it]
loading configuration file generation_config.json from cache at /data/b22cs093/.cache/huggingface/hub/models--microsoft--Phi-3-medium-4k-instruct/snapshots/b64223aaea6fbf273c0c8cd0801d5e732dce8897/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": [
    32000,
    32001,
    32007
  ],
  "pad_token_id": 32000
}

Could not locate the custom_generate/generate.py inside microsoft/Phi-3-medium-4k-instruct.
Device set to use cuda:0
You are not running the flash-attention implementation, expect numerical differences.
loading configuration file config.json from cache at /data/b22cs093/.cache/huggingface/hub/models--microsoft--Phi-3-medium-4k-instruct/snapshots/b64223aaea6fbf273c0c8cd0801d5e732dce8897/config.json
loading configuration file config.json from cache at /data/b22cs093/.cache/huggingface/hub/models--microsoft--Phi-3-medium-4k-instruct/snapshots/b64223aaea6fbf273c0c8cd0801d5e732dce8897/config.json
Model config Phi3Config {
  "architectures": [
    "Phi3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_phi3.Phi3Config",
    "AutoModelForCausalLM": "modeling_phi3.Phi3ForCausalLM"
  },
  "bos_token_id": 1,
  "dtype": "bfloat16",
  "embd_pdrop": 0.0,
  "eos_token_id": 32000,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 17920,
  "max_position_embeddings": 4096,
  "model_type": "phi3",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 10,
  "original_max_position_embeddings": 4096,
  "pad_token_id": 32000,
  "resid_pdrop": 0.0,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "sliding_window": 2047,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.3",
  "use_cache": true,
  "vocab_size": 32064
}

Overriding dtype=None with `dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own dtype to specify the dtype of the remaining non-linear layers or pass dtype=torch.float16 to remove this warning.
loading weights file model.safetensors from cache at /data/b22cs093/.cache/huggingface/hub/models--microsoft--Phi-3-medium-4k-instruct/snapshots/b64223aaea6fbf273c0c8cd0801d5e732dce8897/model.safetensors.index.json
Instantiating Phi3ForCausalLM model under default dtype torch.float16.
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 32000,
  "pad_token_id": 32000
}

target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization
Loading Fine-Tuned Model...
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:02<00:14,  2.90s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:06<00:12,  3.16s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:09<00:09,  3.28s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:12<00:06,  3.29s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:16<00:03,  3.28s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:18<00:00,  2.95s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:18<00:00,  3.09s/it]
loading configuration file generation_config.json from cache at /data/b22cs093/.cache/huggingface/hub/models--microsoft--Phi-3-medium-4k-instruct/snapshots/b64223aaea6fbf273c0c8cd0801d5e732dce8897/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": [
    32000,
    32001,
    32007
  ],
  "pad_token_id": 32000
}

Could not locate the custom_generate/generate.py inside microsoft/Phi-3-medium-4k-instruct.
Device set to use cuda:0
loading file tokenizer.model from cache at /data/b22cs093/.cache/huggingface/hub/models--microsoft--Phi-3-medium-4k-instruct/snapshots/b64223aaea6fbf273c0c8cd0801d5e732dce8897/tokenizer.model
loading file tokenizer.json from cache at /data/b22cs093/.cache/huggingface/hub/models--microsoft--Phi-3-medium-4k-instruct/snapshots/b64223aaea6fbf273c0c8cd0801d5e732dce8897/tokenizer.json
loading file added_tokens.json from cache at /data/b22cs093/.cache/huggingface/hub/models--microsoft--Phi-3-medium-4k-instruct/snapshots/b64223aaea6fbf273c0c8cd0801d5e732dce8897/added_tokens.json
loading file special_tokens_map.json from cache at /data/b22cs093/.cache/huggingface/hub/models--microsoft--Phi-3-medium-4k-instruct/snapshots/b64223aaea6fbf273c0c8cd0801d5e732dce8897/special_tokens_map.json
loading file tokenizer_config.json from cache at /data/b22cs093/.cache/huggingface/hub/models--microsoft--Phi-3-medium-4k-instruct/snapshots/b64223aaea6fbf273c0c8cd0801d5e732dce8897/tokenizer_config.json
loading file chat_template.jinja from cache at None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading configuration file config.json from cache at /data/b22cs093/.cache/huggingface/hub/models--microsoft--Phi-3-medium-4k-instruct/snapshots/b64223aaea6fbf273c0c8cd0801d5e732dce8897/config.json
loading configuration file config.json from cache at /data/b22cs093/.cache/huggingface/hub/models--microsoft--Phi-3-medium-4k-instruct/snapshots/b64223aaea6fbf273c0c8cd0801d5e732dce8897/config.json
Model config Phi3Config {
  "architectures": [
    "Phi3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_phi3.Phi3Config",
    "AutoModelForCausalLM": "modeling_phi3.Phi3ForCausalLM"
  },
  "bos_token_id": 1,
  "dtype": "bfloat16",
  "embd_pdrop": 0.0,
  "eos_token_id": 32000,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 17920,
  "max_position_embeddings": 4096,
  "model_type": "phi3",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 10,
  "original_max_position_embeddings": 4096,
  "pad_token_id": 32000,
  "resid_pdrop": 0.0,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "sliding_window": 2047,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.3",
  "use_cache": true,
  "vocab_size": 32064
}

Overriding dtype=None with `dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own dtype to specify the dtype of the remaining non-linear layers or pass dtype=torch.float16 to remove this warning.
loading weights file model.safetensors from cache at /data/b22cs093/.cache/huggingface/hub/models--microsoft--Phi-3-medium-4k-instruct/snapshots/b64223aaea6fbf273c0c8cd0801d5e732dce8897/model.safetensors.index.json
Instantiating Phi3ForCausalLM model under default dtype torch.float16.
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 32000,
  "pad_token_id": 32000
}

target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization

=== Processing microsoft/Phi-3-medium-4k-instruct ===
Adapter: ./models/phi3-medium-tool-calling-dpo-final
Loading Base Model...
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:02<00:14,  2.87s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:06<00:12,  3.03s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:09<00:09,  3.14s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:12<00:06,  3.25s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:16<00:03,  3.30s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:18<00:00,  3.00s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:18<00:00,  3.09s/it]
loading configuration file generation_config.json from cache at /data/b22cs093/.cache/huggingface/hub/models--microsoft--Phi-3-medium-4k-instruct/snapshots/b64223aaea6fbf273c0c8cd0801d5e732dce8897/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": [
    32000,
    32001,
    32007
  ],
  "pad_token_id": 32000
}

Could not locate the custom_generate/generate.py inside microsoft/Phi-3-medium-4k-instruct.
Device set to use cuda:0
loading configuration file config.json from cache at /data/b22cs093/.cache/huggingface/hub/models--microsoft--Phi-3-medium-4k-instruct/snapshots/b64223aaea6fbf273c0c8cd0801d5e732dce8897/config.json
loading configuration file config.json from cache at /data/b22cs093/.cache/huggingface/hub/models--microsoft--Phi-3-medium-4k-instruct/snapshots/b64223aaea6fbf273c0c8cd0801d5e732dce8897/config.json
Model config Phi3Config {
  "architectures": [
    "Phi3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_phi3.Phi3Config",
    "AutoModelForCausalLM": "modeling_phi3.Phi3ForCausalLM"
  },
  "bos_token_id": 1,
  "dtype": "bfloat16",
  "embd_pdrop": 0.0,
  "eos_token_id": 32000,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 17920,
  "max_position_embeddings": 4096,
  "model_type": "phi3",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "num_key_value_heads": 10,
  "original_max_position_embeddings": 4096,
  "pad_token_id": 32000,
  "resid_pdrop": 0.0,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "sliding_window": 2047,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.3",
  "use_cache": true,
  "vocab_size": 32064
}

Overriding dtype=None with `dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own dtype to specify the dtype of the remaining non-linear layers or pass dtype=torch.float16 to remove this warning.
loading weights file model.safetensors from cache at /data/b22cs093/.cache/huggingface/hub/models--microsoft--Phi-3-medium-4k-instruct/snapshots/b64223aaea6fbf273c0c8cd0801d5e732dce8897/model.safetensors.index.json
Instantiating Phi3ForCausalLM model under default dtype torch.float16.
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 32000,
  "pad_token_id": 32000
}

target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization
Loading Fine-Tuned Model...
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:03<00:15,  3.02s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:06<00:13,  3.45s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:09<00:10,  3.34s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:13<00:06,  3.34s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:16<00:03,  3.33s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:18<00:00,  2.99s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:18<00:00,  3.16s/it]
loading configuration file generation_config.json from cache at /data/b22cs093/.cache/huggingface/hub/models--microsoft--Phi-3-medium-4k-instruct/snapshots/b64223aaea6fbf273c0c8cd0801d5e732dce8897/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": [
    32000,
    32001,
    32007
  ],
  "pad_token_id": 32000
}

Could not locate the custom_generate/generate.py inside microsoft/Phi-3-medium-4k-instruct.
Device set to use cuda:0

Report generated: report_outputs.json
